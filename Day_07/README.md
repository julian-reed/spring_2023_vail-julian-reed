# Optimization

## Topics covered in today's module
* Optimization
* Gradident Descent
* Optimizers(SGD, ADAM, etc.)

## Main takeaways from doing today's assignment
Differences and similarities between optimizatization funcions and activiation functions.
How to implement gradient descent without calculatuing it, why it is important

## Challenging, interesting, or exciting aspects of today's assignment
Determining the correct formof gradient descent and specific optimization function may be difficult for more complex models. It was very exciting to try out the different optimization functions and compare their performances as I modified their parameters

## Additional resources used 
DS Stack Exchange post about gradient descent and optimization (https://datascience.stackexchange.com/questions/47142/is-gradient-descent-central-to-every-optimizer)

