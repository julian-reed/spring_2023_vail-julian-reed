# Regularization

## Topics covered in today's module
* L1/L2 Regularization
* Dropout
* Overfitting
* Underfitting

## Main takeaways from doing today's assignment
L1 and L2 are two types of regularization that adds cost to weights, causing those weights to "decay"
Dropout is another form of regularization that works by zeroing out certain layers of an output feature
You can combine L1/L2 and dropout for better effects, but this doesn't always help depending on the size of the dataset

## Challenging, interesting, or exciting aspects of today's assignment
The visualization of L1 and L2 was really helpful and interesting for gaining a better understanding of exactly how they influence the cost function.

## Additional resources used 
TensorFlow reduce_sum documentation: https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum
