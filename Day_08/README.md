# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
Different types of ReLU, softmax, swish, etc.
What vanishing and exploding gradients are, how ReLU and its variations can be used to counter them.

## Challenging, interesting, or exciting aspects of today's assignment
I predict that, like with optimization functions, it will probably require a lot of trial and error to figure out which activation function is best for which scenario.

## Additional resources used 
Google for graphs of each activation function when needed (used for swish, ELU and softmax)
